import os
import glob
import yaml
import torch
import wandb
import torch.nn as nn
from evaluate_model import evaluate_model_with_postprocess
from utils import load_config, ensure_dir
from post_process import load_post_processor_from_config
from modules import SimpleUNet, UNetWithDinoV2Backbone, UncertaintyDepthAnything
from data import DepthDataset
from torch.utils.data import DataLoader, Subset
from torchvision import transforms
from transformers import AutoImageProcessor

# You may choose a different metric for evaluation
BEST_METRIC = "siRMSE"
CONFIG_DIR = "src/configs/postprocess_grid"
BASE_CONFIG_PATH = "src/configs/default.yml"

def build_model(config):
    if config["model"]["type"] == "u_net":
        model = SimpleUNet(
            hidden_channels=config["model"]["hidden_channels"],
            dilation=config["model"]["dilation"],
            num_heads=config["model"]["num_heads"],
            conv_transpose=config["model"]["conv_transpose"]
        )
    elif config["model"]["type"] == "depth_anything":
        model = UncertaintyDepthAnything(
            num_heads=config["model"]["num_heads"],
            include_pretrained_head=config["model"]["include_pretrained_head"]
        )
    elif config["model"]["type"] == "dinov2_backboned_unet":
        model = UNetWithDinoV2Backbone(
            num_heads=config["model"]["num_heads"],
            image_size=(config["data"]["input_size"][0], config["data"]["input_size"][1]),
            conv_transpose=config["model"]["conv_transpose"]
        )
    else:
        raise ValueError(f"Unknown model type: {config['model']['type']}")
    
    return torch.nn.DataParallel(model)

def build_dataloader(config, subset_size=64):
    val_transform = transforms.Compose([
        transforms.Resize((426, 560)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    val_dataset = DepthDataset(
        data_dir=os.path.join(config["data_dir"], "train"),
        list_file=os.path.join(config["data_dir"], "train_list.txt"),
        transform=val_transform,
        target_transform=None,
        has_gt=True
    )

    # Use a subset to speed up evaluation
    val_dataset = Subset(val_dataset, list(range(subset_size)))

    val_loader = DataLoader(
        val_dataset,
        batch_size=config["train"]["batch_size"],
        shuffle=False,
        num_workers=config["data"]["num_workers"],
        pin_memory=config["data"]["pin_memory"]
    )

    return val_loader

def main():
    config_paths = glob.glob(os.path.join(CONFIG_DIR, "*.yml"))
    print(f"Found {len(config_paths)} configs.")

    best_config_path = None
    best_metric_value = float("inf")
    all_results = []

    api = wandb.Api()

    for config_path in config_paths:
        print(f"\nEvaluating config: {config_path}")
        config = load_config(config_path)

        # Setup device
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Load model
        
        wandb_artifact_fullname = config["model"].get("wandb_artifact_fullname")
        assert wandb_artifact_fullname is not None, "Specify `config.wandb_artifact_fullname` to tell this script what to download."

        print("Downloading model artifact")
        model_artifact = api.artifact(wandb_artifact_fullname)
        model_artifact_dir = model_artifact.download()
        print("Model artifact downloaded to", model_artifact_dir)

        model = build_model(config)
        model = model.to(device)
        print(f"Using device: {device}")

        state_dict = torch.load(os.path.join(model_artifact_dir, 'best_model.pth'), map_location=device)
        model.load_state_dict(state_dict)

        # Load dataloader
        val_loader = build_dataloader(config)

        # Load postprocessor
        post_processor = load_post_processor_from_config(config)

        # Output directory for visuals
        eval_output_dir = os.path.join(config["output_dir"], f"{config['logging']['run_name']}_eval")
        ensure_dir(eval_output_dir)

        # Run evaluation
        metrics = evaluate_model_with_postprocess(model, post_processor, val_loader, device, eval_output_dir)
        score = metrics[BEST_METRIC]

        all_results.append((config_path, score))
        print(f"{BEST_METRIC} for {os.path.basename(config_path)}: {score:.4f}")

        if score < best_metric_value:
            best_metric_value = score
            best_config_path = config_path

    print("\n==== Grid Search Summary ====")
    for path, score in sorted(all_results, key=lambda x: x[1]):
        print(f"{os.path.basename(path)} -> {BEST_METRIC}: {score:.4f}")

    if best_config_path:
        print(f"\nBest config: {best_config_path} with {BEST_METRIC} = {best_metric_value:.4f}")
    else:
        print("No valid config evaluated.")

if __name__ == "__main__":
    main()
